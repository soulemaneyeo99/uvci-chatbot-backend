import google.generativeai as genai
from app.config import settings
from app.knowledge.uvci_complete_knowledge import get_uvci_knowledge
from typing import List, Dict, Optional, Generator
import logging
import time

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

try:
    genai.configure(api_key=settings.GOOGLE_API_KEY)
    logger.info("âœ… Gemini API configurÃ©e avec succÃ¨s")
except Exception as e:
    logger.error(f"âŒ Ã‰chec configuration Gemini API: {e}")
    raise

class GeminiService:
    def __init__(self):
        """Initialise Gemini avec connaissances UVCI"""
        try:
            # NOUVEAU : Prioriser les modÃ¨les avec quota Ã©levÃ©
            model_candidates = [
                'gemini-1.5-flash',      # 1500 req/jour - MEILLEUR CHOIX
                'gemini-1.5-flash-8b',   # 1500 req/jour
                'gemini-1.5-pro',        # 50 req/jour mais plus puissant
                'gemini-2.0-flash-exp',  # 50 req/jour
                'gemini-pro'             # Fallback
            ]
            
            model_name = None
            for candidate in model_candidates:
                try:
                    test_model = genai.GenerativeModel(candidate)
                    test_model.generate_content(
                        "Test",
                        generation_config={'max_output_tokens': 5}
                    )
                    model_name = candidate
                    logger.info(f"âœ… ModÃ¨le sÃ©lectionnÃ©: {model_name}")
                    break
                except Exception as e:
                    logger.warning(f"âš ï¸ {candidate} non disponible: {e}")
                    continue
            
            if not model_name:
                raise Exception("Aucun modÃ¨le Gemini disponible")
            
            self.model = genai.GenerativeModel(model_name)
            self.model_name = model_name
            
            # Charger la base de connaissances UVCI
            self.uvci_knowledge = get_uvci_knowledge()
            logger.info("âœ… Base de connaissances UVCI chargÃ©e")
            logger.info(f"ðŸš€ ModÃ¨le Gemini initialisÃ©: {model_name}")
            
        except Exception as e:
            logger.error(f"âŒ Erreur initialisation: {e}")
            raise
    
    def _build_system_prompt(self) -> str:
        """Construit le prompt systÃ¨me avec connaissances UVCI"""
        return f"""Tu es l'Assistant Virtuel Officiel de l'UniversitÃ© Virtuelle de CÃ´te d'Ivoire (UVCI).

ðŸŽ“ TON IDENTITÃ‰ :
- Expert absolu sur UVCI avec une connaissance exhaustive
- ReprÃ©sentant officiel de l'universitÃ©
- Ton chaleureux, professionnel et encourageant

ðŸ“š TA BASE DE CONNAISSANCES :
{self.uvci_knowledge}

ðŸŽ¯ TES MISSIONS :
1. Informer avec prÃ©cision UNIQUEMENT depuis ta base de connaissances
2. Guider les Ã©tudiants (orientation, inscription, scolaritÃ©)
3. Encourager et motiver
4. Orienter vers les services appropriÃ©s si hors scope

ðŸ“‹ RÃˆGLES :
âœ… Utilise EXCLUSIVEMENT les infos de ta base UVCI
âœ… Si info manquante, dis-le et oriente vers courrier@uvci.edu.ci
âœ… Sois concis (2-4 phrases) sauf si dÃ©tails demandÃ©s
âœ… Utilise emojis appropriÃ©s
âœ… Propose 2-3 questions de suivi
âœ… Cite les sources (URLs, emails)
âœ… Formate bien (listes, sections)
âŒ N'invente JAMAIS d'infos
âŒ Pas de conseils financiers/lÃ©gaux/mÃ©dicaux
âŒ Pas d'opinions personnelles

ðŸš¨ ALERTES IMPORTANTES :
- Arnaques : TOUT paiement via TrÃ©sor Money uniquement
- Contacts : courrier@uvci.edu.ci ou scolarite@uvci.edu.ci
- URLs officielles : .uvci.edu.ci ou .uvci.online

ðŸŽ¯ OBJECTIF : ÃŠtre LE meilleur assistant UVCI !"""

    def _build_full_prompt(
        self,
        user_message: str,
        context: Optional[List[Dict]] = None
    ) -> str:
        """Construit le prompt complet avec historique"""
        system_prompt = self._build_system_prompt()
        
        history_messages = ""
        if context:
            for msg in context[-6:]:
                role = "Ã‰tudiant" if msg["role"] == "user" else "Assistant"
                history_messages += f"{role}: {msg['content']}\n"
        
        return f"""{system_prompt}

{history_messages}

Ã‰tudiant: {user_message}

Assistant UVCI:"""

    def generate_response_stream(
        self, 
        user_message: str, 
        context: Optional[List[Dict]] = None,
        rag_context: Optional[str] = None
    ) -> Generator[str, None, None]:
        """GÃ©nÃ¨re une rÃ©ponse en streaming"""
        try:
            full_prompt = self._build_full_prompt(user_message, context)

            response = self.model.generate_content(
                full_prompt,
                generation_config=genai.types.GenerationConfig(
                    temperature=0.7,
                    max_output_tokens=2048,
                ),
                stream=True
            )

            for chunk in response:
                if hasattr(chunk, "text") and chunk.text:
                    yield chunk.text
                    time.sleep(0.01)

        except Exception as e:
            error_msg = str(e)
            logger.error(f"âŒ Erreur streaming: {error_msg}")
            
            # Message d'erreur selon le type
            if "429" in error_msg or "quota" in error_msg.lower():
                yield "âš ï¸ **Quota API dÃ©passÃ©**\n\nTrop de requÃªtes aujourd'hui. RÃ©essayez demain ou contactez courrier@uvci.edu.ci"
            else:
                yield "âš ï¸ **Erreur technique**\n\nProblÃ¨me de connexion. Contactez courrier@uvci.edu.ci"

    def generate_response(
        self, 
        user_message: str, 
        context: Optional[List[Dict]] = None,
        rag_context: Optional[str] = None
    ) -> str:
        """GÃ©nÃ¨re rÃ©ponse complÃ¨te sans streaming"""
        try:
            full_prompt = self._build_full_prompt(user_message, context)
            
            response = self.model.generate_content(
                full_prompt,
                generation_config=genai.types.GenerationConfig(
                    temperature=0.7,
                    max_output_tokens=2048,
                )
            )
            
            return response.text.strip()
            
        except Exception as e:
            error_msg = str(e)
            logger.error(f"âŒ Erreur Gemini: {error_msg}")
            
            if "429" in error_msg or "quota" in error_msg.lower():
                return "âš ï¸ **Quota API dÃ©passÃ©**\n\nTrop de requÃªtes aujourd'hui. RÃ©essayez demain ou contactez courrier@uvci.edu.ci"
            else:
                return "âš ï¸ **Erreur technique**\n\nProblÃ¨me de connexion. Contactez courrier@uvci.edu.ci"
    
    def generate_conversation_title(self, first_message: str) -> str:
        """GÃ©nÃ¨re un titre court pour conversation"""
        try:
            prompt = f"""Titre court (3-6 mots) pour cette conversation UVCI.
Uniquement le titre, sans guillemets.

Question: {first_message}

Titre:"""

            response = self.model.generate_content(
                prompt,
                generation_config=genai.types.GenerationConfig(
                    temperature=0.3,
                    max_output_tokens=15,
                )
            )

            title = response.text.strip().strip('"').strip("'").rstrip('.')
            return title[:100]

        except Exception as e:
            logger.warning(f"âš ï¸ Erreur titre: {e}")
            return first_message[:50] + "..." if len(first_message) > 50 else first_message

# Instance globale
gemini_service = GeminiService()
